{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Astronomer Starship can send your Airflow workloads to new places! </p>"},{"location":"#what-is-it","title":"What is it?","text":"<p>Starship is a utility to migrate Airflow metadata such as Airflow Variables, Connections, Environment Variables, Pools, and DAG History between two Airflow instances.</p> <p> </p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install astronomer-starship\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<ol> <li>Create a Workspace in Astro or Software to hold Astro Deployments</li> <li>Create an Astro Deployment matching the source Airflow deployment configuration as possible</li> <li>Run <code>astro dev init</code> with the Astro CLI to create a Astro Project locally in your terminal</li> <li>Add any DAGs to the <code>/dags</code> folder in the Astro Project</li> <li>Complete any additional setup required to convert your existing Airflow deployment to an Astro Project</li> <li>Install Starship (and any additional Python Dependencies) to the Astro Project</li> <li>Install Starship to your existing Airflow Deployment</li> <li>Deploy the Astro Project to the Astro Deployment with <code>astro deploy</code></li> <li>In the Airflow UI of the source Airflow deployment, navigate to the new <code>Astronomer</code> menu and select the <code>Migration Tool \ud83d\ude80</code> option</li> <li>Follow the UI prompts to migrate, or if needed, look at the instructions to use the Operator</li> </ol>"},{"location":"#compatability","title":"Compatability","text":"Source Compatible Airflow 1 \u274c GCC 1 - Airflow 2.x Operator GCC 2 - Airflow 2.x \u2705 MWAA v2.0.2 Operator MWAA \u2265 v2.2.2 \u2705 OSS Airflow VM \u2705 Astronomer Products \u2705"},{"location":"#faq","title":"FAQ","text":"<ul> <li> <p>I'm on Airflow 1, can I use Starship?</p> <p>No, Starship is only compatible with Airflow 2.x and above, see Compatibility</p> </li> <li> <p>I'm on Airflow&gt;=2.7 and can't test connections?</p> </li> </ul> <p>You must have <code>AIRFLOW__CORE__TEST_CONNECTION</code> set. See notes here</p> <ul> <li> <p>I'm using Google Cloud Composer 2.x and Airflow 2.x and do not see the <code>Astronomer</code> menu and/or the Starship Airflow Plugin?</p> <p>Run the following to ensure you are a privileged user. <pre><code>gcloud config set project &lt;PROJECT_NAME&gt;\ngcloud composer environments run &lt;ENVIRONMENT_NAME&gt; --location &lt;LOCATION&gt; users add-role -- -e &lt;USER_EMAIL&gt; -r Admin\n</code></pre></p> </li> </ul>"},{"location":"#security-notice","title":"Security Notice","text":"<p>This project is an Airflow Plugin that adds custom API routes. Ensure your environments are correctly secured.</p> <p>Artwork Starship logo by Lorenzo used with permission from The Noun Project under Creative Commons.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/#airflow-version","title":"Airflow Version","text":"<p>Returns the version of Airflow that the Starship API is connected to.</p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.airflow_version--get-starshipapiairflow_version","title":"<code>GET /starship/api/airflow_version</code>","text":"<p>Parameters: None</p> <p>Response: <pre><code>OK\n</code></pre></p>"},{"location":"api/#health","title":"Health","text":"<p>Returns the health of the Starship API</p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.health--get-starshipapihealth","title":"<code>GET /starship/api/health</code>","text":"<p>Parameters: None</p> <p>Response: <pre><code>OK\n</code></pre></p>"},{"location":"api/#environment-variables","title":"Environment Variables","text":"<p>Get the Environment Variables, which may be used to set Airflow Connections, Variables, or Configurations</p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.env_vars--get-starshipapienv_vars","title":"<code>GET /starship/api/env_vars</code>","text":"<p>Parameters: None</p> <p>Response: <pre><code>{\n    \"FOO\": \"bar\",\n    \"AIRFLOW__CORE__SQL_ALCHEMY_CONN\": \"sqlite:////usr/local/airflow/airflow.db\",\n    ...\n}\n</code></pre></p>"},{"location":"api/#variable","title":"Variable","text":"<p>Get Variables or set a Variable</p> <p>Model: <code>airflow.models.Variable</code></p> <p>Table: <code>variable</code></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.variables--get-starshipapivariable","title":"<code>GET /starship/api/variable</code>","text":"<p>Parameters: None</p> <p>Response: <pre><code>[\n    {\n        \"key\": \"key\",\n        \"val\": \"val\",\n        \"description\": \"My Var\"\n    },\n    ...\n]\n</code></pre></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.variables--post-starshipapivariable","title":"<code>POST /starship/api/variable</code>","text":"<p>Parameters: JSON</p> Field (*=Required) Version Type Example key* str key val* str val description str My Var <p>Response: List of Variables, as <code>GET</code> Response</p>"},{"location":"api/#pools","title":"Pools","text":"<p>Get Pools or set a Pool</p> <p>Model: <code>airflow.models.Pool</code></p> <p>Table: <code>pools</code></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.pools--get-starshipapipools","title":"GET /starship/api/pools","text":"<p>Parameters: None</p> <p>Response: <pre><code>[\n    {\n        \"name\": \"my_pool\",\n        \"slots\": 5,\n        \"description\": \"My Pool\n    },\n    ...\n]\n</code></pre></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.pools--post-starshipapipools","title":"POST /starship/api/pools","text":"<p>Parameters: JSON</p> Field (*=Required) Version Type Example name* str my_pool slots* int 5 description str My Pool include_deferred* &gt;=2.7 bool True <p>Response: List of Pools, as <code>GET</code> Response</p>"},{"location":"api/#connections","title":"Connections","text":"<p>Get Connections or set a Connection</p> <p>Model: <code>airflow.models.Connections</code></p> <p>Table: <code>connection</code></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.connections--get-starshipapiconnection","title":"<code>GET /starship/api/connection</code>","text":"<p>Parameters: None</p> <p>Response: <pre><code>[\n    {\n        \"conn_id\": \"my_conn\",\n        \"conn_type\": \"http\",\n        \"host\": \"localhost\",\n        \"port\": \"1234\",\n        \"schema\": \"https\",\n        \"login\": \"user\",\n        \"password\": \"foobar\",  # pragma: allowlist secret\n        \"extra\": \"{}\",\n        \"conn_type\": \"http\",\n        \"conn_type\": \"http\",\n        \"conn_type\": \"http\",\n        \"description\": \"My Var\"\n    },\n    ...\n]\n</code></pre></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.connections--post-starshipapiconnection","title":"<code>POST /starship/api/connection</code>","text":"<p>Parameters: JSON</p> Field (*=Required) Version Type Example conn_id* str my_conn conn_type* str http host str localhost port int 1234 schema str https login str user password str ** extra dict {} description str My Conn <p>Response: List of Connections, as <code>GET</code> Response</p>"},{"location":"api/#dags","title":"DAGs","text":"<p>Get DAG or pause/unpause a DAG</p> <p>Model: <code>airflow.models.DagModel</code></p> <p>Table: <code>dags</code></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.dags--get-starshipapidags","title":"<code>GET /starship/api/dags</code>","text":"<p>Parameters: None</p> <p>Response: <pre><code>[\n    {\n        \"dag_id\": \"dag_0\",\n        \"schedule_interval\": \"0 0 * * *\",\n        \"is_paused\": true,\n        \"fileloc\": \"/usr/local/airflow/dags/dag_0.py\",\n        \"description\": \"My Dag\",\n        \"owners\": \"user\",\n        \"tags\": [\"tag1\", \"tag2\"],\n        \"dag_run_count\": 2,\n    },\n    ...\n]\n</code></pre></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.dags--patch-starshipapidags","title":"<code>PATCH /starship/api/dags</code>","text":"<p>Parameters: JSON</p> Field (*=Required) Version Type Example dag_id* str dag_0 is_paused* bool true <pre><code>{\n    \"dag_id\": \"dag_0\",\n    \"is_paused\": true\n}\n</code></pre>"},{"location":"api/#dag-runs","title":"DAG Runs","text":"<p>Get DAG Runs or set DAG Runs</p> <p>Model: <code>airflow.models.DagRun</code></p> <p>Table: <code>dag_run</code></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.dag_runs--get-starshipapidag_runs","title":"<code>GET /starship/api/dag_runs</code>","text":"<p>Parameters: Args</p> Field (*=Required) Version Type Example dag_id* str dag_0 limit int 10 offset int 0 <p>Response: <pre><code>[\n    {\n        \"dag_id\": \"dag_0\",\n        \"queued_at\": \"1970-01-01T00:00:00+00:00\",\n        \"execution_date\": \"1970-01-01T00:00:00+00:00\",\n        \"start_date\": \"1970-01-01T00:00:00+00:00\",\n        \"end_date\": \"1970-01-01T00:00:00+00:00\",\n        \"state\": \"SUCCESS\",\n        \"run_id\": \"manual__1970-01-01T00:00:00+00:00\",\n        \"creating_job_id\": 123,\n        \"external_trigger\": true,\n        \"run_type\": \"manual\",\n        \"conf\": None,\n        \"data_interval_start\": \"1970-01-01T00:00:00+00:00\",\n        \"data_interval_end\": \"1970-01-01T00:00:00+00:00\",\n        \"last_scheduling_decision\": \"1970-01-01T00:00:00+00:00\",\n        \"dag_hash\": \"....\"\n    },\n    ...\n]\n</code></pre></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.dag_runs--post-starshipapidag_runs","title":"<code>POST /starship/api/dag_runs</code>","text":"<p>Parameters: JSON</p> Field (*=Required) Version Type Example dag_runs list[DagRun] [ ... ] <pre><code>{\n    \"dag_runs\": [ ... ]\n}\n</code></pre> <p>DAG Run:</p> Field (*=Required) Version Type Example dag_id* str dag_0 queued_at date 1970-01-01T00:00:00+00:00 execution_date* date 1970-01-01T00:00:00+00:00 start_date date 1970-01-01T00:00:00+00:00 end_date date 1970-01-01T00:00:00+00:00 state str SUCCESS run_id* str manual__1970-01-01T00:00:00+00:00 creating_job_id int 123 external_trigger bool true run_type* str manual conf dict {} data_interval_start &gt;2.1 date 1970-01-01T00:00:00+00:00 data_interval_end &gt;2.1 date 1970-01-01T00:00:00+00:00 last_scheduling_decision date 1970-01-01T00:00:00+00:00 dash_hash str ... clean_number &gt;=2.8 int 0"},{"location":"api/#task-instances","title":"Task Instances","text":"<p>Get TaskInstances or set TaskInstances</p> <p>Model: <code>airflow.models.TaskInstance</code></p> <p>Table: <code>task_instance</code></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.task_instances--get-starshipapitask_instances","title":"<code>GET /starship/api/task_instances</code>","text":"<p>Parameters: Args</p> Field (*=Required) Version Type Example dag_id* str dag_0 limit int 10 offset int 0 <p>Response: <pre><code>{\n    \"task_instances\": [\n        {\n            \"task_instances\": []\n            \"run_id\": \"manual__1970-01-01T00:00:00+00:00\",\n            \"queued_at\": \"1970-01-01T00:00:00+00:00\",\n            \"execution_date\": \"1970-01-01T00:00:00+00:00\",\n            \"start_date\": \"1970-01-01T00:00:00+00:00\",\n            \"end_date\": \"1970-01-01T00:00:00+00:00\",\n            \"state\": \"SUCCESS\",\n            \"creating_job_id\": 123,\n            \"external_trigger\": true,\n            \"run_type\": \"manual\",\n            \"conf\": None,\n            \"data_interval_start\": \"1970-01-01T00:00:00+00:00\",\n            \"data_interval_end\": \"1970-01-01T00:00:00+00:00\",\n            \"last_scheduling_decision\": \"1970-01-01T00:00:00+00:00\",\n            \"dag_hash\": \"....\"\n        },\n        ...\n    ],\n    \"dag_run_count\": 2,\n}\n</code></pre></p>"},{"location":"api/#astronomer_starship.starship_api.StarshipApi.task_instances--post-starshipapitask_instances","title":"<code>POST /starship/api/task_instances</code>","text":"<p>Parameters: JSON</p> Field (*=Required) Version Type Example task_instances list[TaskInstance] [ ... ] <pre><code>{\n    \"task_instances\": [ ... ]\n}\n</code></pre> <p>Task Instance:</p> Field (*=Required) Version Type Example dag_id* str dag_0 run_id* &gt;2.1 str manual__1970-01-01T00:00:00+00:00 task_id* str task_0 map_index* &gt;2.2 int -1 execution_date* &lt;=2.1 date 1970-01-01T00:00:00+00:00 start_date date 1970-01-01T00:00:00+00:00 end_date date 1970-01-01T00:00:00+00:00 duration float 0.0 max_tries int 2 hostname str host unixname str unixname job_id int 123 pool* str default_pool pool_slots int 1 queue str queue priority_weight int 1 operator str BashOperator queued_dttm date 1970-01-01T00:00:00+00:00 queued_by_job_id int 123 pid int 123 external_executor_id int trigger_id &gt;2.1 str trigger_timeout &gt;2.1 date 1970-01-01T00:00:00+00:00 executor_config str"},{"location":"operator/","title":"Operator","text":"<p>The Starship Operator should be used in instances where the Airflow Webserver is unable to correctly host a Plugin.</p> <p>The <code>AstroMigrationOperator</code> should be used if migrating from a Google Cloud Composer 1 (with Airflow 2.x) or MWAA v2.0.2 environment. These environments do not support webserver plugins and will require using the <code>AstroMigrationOperator</code> to migrate data.</p>"},{"location":"operator/#installation","title":"Installation","text":"<p>Add the following line to your <code>requirements.txt</code> in your source environment:</p> <pre><code>astronomer-starship==1.2.1\n</code></pre>"},{"location":"operator/#usage","title":"Usage","text":"<ol> <li> <p>Add the following DAG to your source environment:</p> dags/astronomer_migration_dag.py<pre><code>from airflow import DAG\n\nfrom astronomer.starship.operators import AstroMigrationOperator\nfrom datetime import datetime\n\nwith DAG(\ndag_id=\"astronomer_migration_dag\",\nstart_date=datetime(2020, 8, 15),\nschedule_interval=None,\n) as dag:\n\nAstroMigrationOperator(\ntask_id=\"export_meta\",\ndeployment_url='{{ dag_run.conf[\"deployment_url\"] }}',\ntoken='{{ dag_run.conf[\"astro_token\"] }}',\n)\n</code></pre> </li> <li> <p>Deploy this DAG to your source Airflow environment, configured as described in the Configuration section below</p> </li> <li>Once the DAG is available in the Airflow UI, click the \"Trigger DAG\" button, then click \"Trigger DAG w/ config\", and input the following in the configuration dictionary:</li> <li><code>astro_token</code>: To retrieve anf Astronomer token, navigate to cloud.astronomer.io/token and log in using your Astronomer credentials</li> <li><code>deployment_url</code>: To retrieve a deployment URL - navigate to the Astronomer Airlow deployment that you'd like to migrate to in the Astronomer UI, click <code>Open Airflow</code> and copy the page URL (excluding <code>/home</code> on the end of the URL)<ul> <li>For example, if your deployment URL is <code>https://astronomer.astronomer.run/abcdt4ry/home</code>, you'll use <code>https://astronomer.astronomer.run/abcdt4ry</code></li> </ul> </li> <li> <p>The config dictionary used when triggering the DAG should be formatted as:</p> <p><pre><code>{\n \"deployment_url\": \"your-deployment-url\",\n \"astro_token\": \"your-astro-token\"\n}\n</code></pre> 5. Once the DAG successfully runs, your connections, variables, and environment variables should all be migrated to Astronomer</p> </li> </ol>"},{"location":"operator/#configuration","title":"Configuration","text":"<p>The <code>AstroMigrationOperator</code> can be configured as follows:</p> <ul> <li><code>variables_exclude_list</code>: List the individual Airflow Variables which you do not want to be migrated. Any Variables not listed will be migrated to the desination Airflow deployment.</li> <li><code>connection_exclude_list</code>: List the individual Airflow Connections which you do not want to be migrated. Any Variables not listed will be migrated to the desination Airflow deployment.</li> <li> <p><code>env_include_list</code>: List the individual Environment Variables which you do want to be migrated. Only the Environment Variables listed will be migrated to the desination Airflow deployment. None are migrated by default.</p> <pre><code>AstroMigrationOperator(\n    task_id=\"export_meta\",\n    deployment_url='{{ dag_run.conf[\"deployment_url\"] }}',\n    token='{{ dag_run.conf[\"astro_token\"] }}',\n    variables_exclude_list=[\"some_var_1\"],\n    connection_exclude_list=[\"some_conn_1\"],\n    env_include_list=[\"FOO\", \"BAR\"],\n)\n</code></pre> </li> </ul>"},{"location":"migration_source/gcc/","title":"Google Cloud Composer","text":""},{"location":"migration_source/gcc/#compatability","title":"Compatability","text":"Source Compatible Airflow 1 \u274c GCC 1 - Airflow 2.x Operator GCC 2 - Airflow 2.x \u2705"},{"location":"migration_source/gcc/#notes","title":"Notes","text":"<p>You must be an Admin to see Plugins on GCC.</p>"},{"location":"migration_source/gcc/#installation","title":"Installation","text":"<ol> <li>Navigate to your Environments</li> <li>Go to PyPi Packages     </li> <li>Click <code>+ Add Package</code> and put <code>astronomer-starship</code> under <code>Package name</code> </li> </ol>"},{"location":"migration_source/gcc/#faq","title":"FAQ","text":"<ul> <li> <p>I'm using Google Cloud Composer 2.x and Airflow 2.x and do not see the <code>Astronomer</code> menu and/or the Starship Airflow Plugin?</p> <p>Run the following to ensure you are a privileged user. <pre><code>gcloud config set project &lt;PROJECT_NAME&gt;\ngcloud composer environments run &lt;ENVIRONMENT_NAME&gt; --location &lt;LOCATION&gt; users add-role -- -e &lt;USER_EMAIL&gt; -r Admin\n</code></pre></p> </li> </ul>"},{"location":"migration_source/mwaa/","title":"(AWS) Managed Apache Airflow","text":""},{"location":"migration_source/mwaa/#compatability","title":"Compatability","text":"Source Compatible Airflow 1 \u274c MWAA v2.0.2 Operator MWAA \u2265 v2.2.2 \u2705"},{"location":"migration_source/mwaa/#installation","title":"Installation","text":"<ol> <li>Navigate to your Environments</li> <li>Download your existing <code>requirements.txt</code></li> <li>Add <code>astronomer-starship</code> to the file, save it, and re-upload it to S3</li> <li>Click <code>Edit</code>, and pick the newer version of your Requirements File     </li> <li>Click <code>Next</code>, then eventually <code>Save</code>, and then wait for your deployment to restart and dependencies to install</li> </ol>"}]}